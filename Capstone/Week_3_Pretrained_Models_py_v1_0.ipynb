{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2925352-cf72-4358-8b66-cc138aa866e9"
      },
      "source": [
        "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
        "\n",
        "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3a65f9f-a2f6-4638-86a0-9bdca278db4d"
      },
      "source": [
        "## Objective\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02a8abe1-349c-439c-9987-6e322b987c98"
      },
      "source": [
        "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69e63421-70c1-4b3a-848c-88cdce408333"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<font size = 3>\n",
        "    \n",
        "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
        "2. <a href=\"#item32\">Download Data</a>  \n",
        "3. <a href=\"#item33\">Define Global Constants</a>  \n",
        "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
        "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
        "\n",
        "</font>\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3444ad8-b64d-41bb-8327-b36d29fc8d9d"
      },
      "source": [
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fee1c17-a8f8-4455-ae42-739000f416d3"
      },
      "source": [
        "<a id='item31'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf2de36-c0e6-43e5-a0db-fadb1c7cc2c4"
      },
      "source": [
        "## Import Libraries and Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "097f5b90-6571-4d5e-9023-d75d3a21c48a"
      },
      "source": [
        "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eabd8aa-a92f-4cad-a145-891a1129b2bb"
      },
      "outputs": [],
      "source": [
        "#import skillsnetwork"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd0535a-e627-47a1-b903-2a52c2d08ab3"
      },
      "source": [
        "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fea0e051-001b-488c-b2c5-87a682b745b6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f27d5383-338e-4439-9627-6f92a11f4ea8"
      },
      "source": [
        "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library => use tensorflow instead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e49e0408-acba-4e62-b4bf-cec7316c18a8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "598a265a-0e24-4d65-b693-4fb6ee385b78"
      },
      "source": [
        "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c5c4960-0346-41bb-b9a7-3fbd3eeba3b5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80a1e013-fd42-4cd0-8c05-cc386374e1bd"
      },
      "source": [
        "<a id='item32'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c45f6e6-18c7-4bea-b0ab-f9102799a26a"
      },
      "source": [
        "## Download Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b770f4e-954a-4de2-8759-7b49d6ffeb61"
      },
      "source": [
        "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSZGO8ZGrp2z",
        "outputId": "aa140e1e-c6d1-4f76-e97c-644b181b988b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed.\n"
          ]
        }
      ],
      "source": [
        "## get the data\n",
        "#await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL of the file\n",
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the file locally\n",
        "with open(\"concrete_data_week3.zip\", \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(\"Download completed.\")\n",
        "import zipfile\n",
        "path = \"/content/concrete_data_week3.zip\"\n",
        "with zipfile.ZipFile(path,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ed2a99-c7de-482f-bd42-9ceece8d9fe4"
      },
      "source": [
        "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab34b495-4e94-4d32-b8da-e624146cb339"
      },
      "source": [
        "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0515abb-5932-4aee-9117-5cef6ce5a42f"
      },
      "source": [
        "<a id='item33'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9f1629b-6a7b-417b-8211-4cad9f541da0"
      },
      "source": [
        "## Define Global Constants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ac6ae5-3a91-488f-9b08-eb08da56bdb2"
      },
      "source": [
        "Here, we will define constants that we will be using throughout the rest of the lab.\n",
        "\n",
        "1. We are obviously dealing with two classes, so *num_classes* is 2.\n",
        "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
        "3. We will training and validating the model using batches of 100 images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39b869e6-88ef-4d50-a337-c072fa384587"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "\n",
        "image_resize = 224\n",
        "\n",
        "batch_size_training = 100\n",
        "batch_size_validation = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "206726de-dc37-4a5b-aa9a-2daf6908b586"
      },
      "source": [
        "<a id='item34'></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ff9ec73-ae0c-4105-baa5-b4bdde215667"
      },
      "source": [
        "## Construct ImageDataGenerator Instances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32a656dd-0656-4a1c-80d9-65859faa7649"
      },
      "source": [
        "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e97a55ab-4e21-42bd-9a5d-60124e306fec"
      },
      "outputs": [],
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2b4f25a-343e-4914-84ad-0dabdc5190de"
      },
      "source": [
        "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8625422-6d7b-4e3f-b996-211e88e773c2",
        "outputId": "02bcf9e0-3e50-4415-c37d-97defb5a055b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10001 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = data_generator.flow_from_directory(\n",
        "    '/content/concrete_data_week3/train',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaa0f25a-9dae-4d5a-bc85-648c8f371e57"
      },
      "source": [
        "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bc96524-5a08-40ce-84f6-2badc2e80c04"
      },
      "source": [
        "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f817647-eaad-4d68-9af9-222e0dd1b35f",
        "outputId": "9634194f-7bd8-420d-ba63-c306fed6cb0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5001 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "## Type your answer here\n",
        "data_generator_val = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        ")\n",
        "val_generator = data_generator_val.flow_from_directory(\n",
        "    '/content/concrete_data_week3/valid',\n",
        "    target_size=(image_resize, image_resize),\n",
        "    batch_size=batch_size_training,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05f3e94-8b47-4637-a60c-226113cb2522"
      },
      "source": [
        "## Build, Compile and Fit Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25e626fb-98c7-4b08-a06c-b780f6bba9ab"
      },
      "source": [
        "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebd441e7-2c95-4c70-8e21-3643636fc2ea"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b74f39f-5b89-47ff-be05-fca035502ddc"
      },
      "source": [
        "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d339754c-09cb-471a-890b-b17ce2e9e433"
      },
      "outputs": [],
      "source": [
        "model.add(ResNet50(\n",
        "    include_top=False,\n",
        "    pooling='avg',\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a9a03d8-84e8-49fa-aa58-8d0e20a0ffc5"
      },
      "source": [
        "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "206145b8-45f9-43c6-9db6-1ad4bd073d92"
      },
      "outputs": [],
      "source": [
        "model.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c68b992-8724-4884-aa25-d8516faa16bd"
      },
      "source": [
        "You can access the model's layers using the *layers* attribute of our model object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04386088-61ae-48d8-8a71-295e99cd9b5a",
        "outputId": "9fc787a2-41e6-4746-b8de-aea61bcc337a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.src.engine.functional.Functional at 0x788d643c0820>,\n",
              " <keras.src.layers.core.dense.Dense at 0x788d643c16c0>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cdfa175-97d4-416b-a78f-930f3f971362"
      },
      "source": [
        "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6eb32d-0c61-4cb6-be0f-15e63ac9945b"
      },
      "source": [
        "You can access the ResNet50 layers by running the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89b6058d-d5bc-4150-a5c2-8b3000f39865",
        "outputId": "8897abd8-b5ce-4090-ab88-e4dd60026b0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.src.engine.input_layer.InputLayer at 0x788d8416e770>,\n",
              " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x788d8416fbe0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d8416d5a0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc454c7f0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d647897b0>,\n",
              " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x788dc43e1570>,\n",
              " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x788d6478be50>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d647898a0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d64788e80>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6478bc70>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d64789090>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc467b340>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788dc4679ff0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d647880d0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788dc467b100>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d64788fa0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc467a1a0>,\n",
              " <keras.src.layers.merging.add.Add at 0x788dc467b070>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788dc4679d20>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788dc4678790>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc4679000>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d8415b4f0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d84159630>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc4679690>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d84158a90>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d647081f0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d6470b520>,\n",
              " <keras.src.layers.merging.add.Add at 0x788d84159d80>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6470bfd0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6470acb0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d647095a0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788dc454c730>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6470a110>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d64708400>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6470ac20>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788dc458d150>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc458dde0>,\n",
              " <keras.src.layers.merging.add.Add at 0x7890340b0820>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7890340b2740>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788dc458f7c0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc458f490>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7890340b3490>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788dc458e590>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc458e8c0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x7890340b14e0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6470aec0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d84158b50>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc458f340>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d84158b20>,\n",
              " <keras.src.layers.merging.add.Add at 0x788d8415a080>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d647080d0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788dc4678310>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc458fa30>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d64708190>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d245615d0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d24561e70>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d24562b30>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d24562320>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644d61a0>,\n",
              " <keras.src.layers.merging.add.Add at 0x788d6470bdf0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d644d41c0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d644d7ca0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644d5a80>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d644d4580>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d644d7b20>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644d4c70>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de42f32b0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de42f37f0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de42f0760>,\n",
              " <keras.src.layers.merging.add.Add at 0x78900806bee0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x789008069030>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de42f0130>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de42f0940>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de42f3010>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de42f04c0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de42f0580>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de436bdc0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de43694b0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de436b010>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de436be20>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4368be0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4368040>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x789008068a00>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4368af0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d24561930>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de42f3520>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788dc4678f70>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de436a8f0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de42f0a60>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de436a0b0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788dc46785e0>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de42f30d0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788e041e1e40>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788e041e1240>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644d4490>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788e041e1a80>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788e041e0e80>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788e041e14b0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de43392a0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4339330>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4338df0>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de4338910>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de433a500>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4339e70>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de43388b0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4338070>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4462170>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4461270>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4461420>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4463340>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4463d00>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de4461000>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de448bf70>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de448bdc0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4489330>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4488af0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de448ab30>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4488520>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de448a0b0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de44893f0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de448af50>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de4489840>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de44893c0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de448bb80>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4463400>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4339210>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4338c70>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788e041e2050>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de42f09d0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de448b3a0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de433a410>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de448bc40>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4524a60>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4524df0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4527fd0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788de4527be0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4527fa0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788de4525a20>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d241c5bd0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d241c5e40>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d241c6a40>,\n",
              " <keras.src.layers.merging.add.Add at 0x788d241c6d40>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d241c68c0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d24135930>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d24134100>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d24135180>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d24136b90>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d24137cd0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d24135990>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d241c76a0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6449d0f0>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644d4430>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d24134d60>,\n",
              " <keras.src.layers.merging.add.Add at 0x788d241356f0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6449d2a0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6449fa90>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d6449df90>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6449de40>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6449c340>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d6449d7e0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6449d1b0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d6449ed70>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d241c7b80>,\n",
              " <keras.src.layers.merging.add.Add at 0x788d241361d0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d6449c280>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788de4527280>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788e041e3700>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d245600a0>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d644f7430>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644f40d0>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d644f4a90>,\n",
              " <keras.src.layers.convolutional.conv2d.Conv2D at 0x788d644f5780>,\n",
              " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x788d644f67d0>,\n",
              " <keras.src.layers.merging.add.Add at 0x788de433ba60>,\n",
              " <keras.src.layers.core.activation.Activation at 0x788d643c1330>,\n",
              " <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x788d644f43d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "model.layers[0].layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAmP8F0JwgPo",
        "outputId": "77d1e85c-5572-4ad7-805d-46828d0551a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.layers.core.dense.Dense at 0x788d643c16c0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "model.layers[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec6a949d-86d5-4b23-a165-f5545012da44"
      },
      "source": [
        "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d3dddb1-6c60-4909-8781-43bbe3f49614"
      },
      "outputs": [],
      "source": [
        "model.layers[0].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG2_NlgSyadD",
        "outputId": "9bc1555c-da14-4613-b165-09bbb1ad4b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ],
      "source": [
        "batch = np.random.random((1, 224, 224, 3))\n",
        "\n",
        "# Perform a dummy pass to initialize shapes\n",
        "try:\n",
        "    model.predict(batch)\n",
        "except Exception as e:\n",
        "    print(f\"Prediction Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2f75fd6-fc93-474d-8606-4e2215bcdebe"
      },
      "source": [
        "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cdba85e-ea71-4b92-8aec-9073aa24f74d",
        "outputId": "b5285c96-118e-4caa-aaef-bdfd2ec30196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 2048)              23587712  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23591810 (90.00 MB)\n",
            "Trainable params: 4098 (16.01 KB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faab0ab4-bf53-418b-b257-2dd759ed5af7"
      },
      "source": [
        "Next we compile our model using the **adam** optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0a432df-923b-4342-9b80-40f56b26c3c8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d4e333b-01f8-42e0-beb8-c7d324714a09"
      },
      "source": [
        "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db8c3174-911e-48e6-9328-afd9f22d7e8d"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch_training = len(train_generator)\n",
        "steps_per_epoch_validation = len(val_generator)\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "048afe9b-09e2-48e2-a656-6d535868ac2c"
      },
      "source": [
        "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f02c9e7-7606-407a-b257-981c89ee1b76",
        "outputId": "035f5b19-b53d-45ed-d2a6-b274f185967c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "101/101 [==============================] - 273s 3s/step - loss: 0.1640 - accuracy: 0.9349 - val_loss: 0.0286 - val_accuracy: 0.9926\n",
            "Epoch 2/2\n",
            "101/101 [==============================] - 271s 3s/step - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.0179 - val_accuracy: 0.9946\n"
          ]
        }
      ],
      "source": [
        "fit_history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=val_generator,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae128d1-021f-449a-baba-1b94a6c567eb"
      },
      "source": [
        "Now that the model is trained, you are ready to start using it to classify images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6225725-15d5-47c5-a8f0-32b5a258695f"
      },
      "source": [
        "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4498940f-4bfe-46e7-b37f-c349c29c4c3b",
        "outputId": "b5895831-35be-4693-dd70-b7d849fe7371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "model.save('/content/restnet50_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb0bb753-a207-4d53-b452-73dd3c55014b"
      },
      "source": [
        "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d197a4f2-fbc4-44a1-a2fa-d7d548cd2d01"
      },
      "source": [
        "### Thank you for completing this lab!\n",
        "\n",
        "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9145ebcc-2a74-4ec9-901a-3d9bf7ff7f2f"
      },
      "source": [
        "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "280eedc9-9bde-4d5d-8358-1c144f18baeb"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975b95f-2884-45db-acf0-5398957793d1"
      },
      "source": [
        "<hr>\n",
        "\n",
        "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}